{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do Modelo Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas scikit-learn xgboost joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importações de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-Processing and evaluation\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from yellowbrick.features import FeatureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruções\n",
    "\n",
    "1. Renomear o arquivo .env_exemplo para somente .env\n",
    "2. Adicionar popular as variaveis conforme o padrão de nomenclatura que voce utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "def read_from_s3(bucket_name:str):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    response = []\n",
    "    for obj in bucket.objects.all():\n",
    "        body = obj.get()['Body'].read()\n",
    "        s=str(body,'utf-8')\n",
    "        data = StringIO(s)\n",
    "        response.append(pd.read_csv(data, index_col=0))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = os.environ['BUCKET_NAME']\n",
    "csv_file_name = os.environ['CSV_FILE_NAME']\n",
    "bucket_layer = os.environ['BUCKET_LAYER']\n",
    "object_name = f'{bucket_layer}/{csv_file_name}'\n",
    "data = read_from_s3(bucket_name)[0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working with data, please uncoment this line for safety, this will generate a backup of the dataframe prior to pre-processing\n",
    "df_backup = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the predictors Columns\n",
    "predictors: List[str] = ['open', 'high', 'low', 'close', 'volume', 'close_ratio_2',\n",
    "       'edit_2', 'trend_2', 'close_ratio_7', 'edit_7', 'trend_7',\n",
    "       'close_ratio_30', 'edit_30', 'trend_30', 'close_ratio_60', 'edit_60',\n",
    "       'trend_60', 'close_ratio_365', 'edit_365', 'trend_365', 'edit_count', \n",
    "       'sentiment', 'neg_sentiment', 'fng_index']\n",
    "\n",
    "map = {'Neutral': 0, 'Greed': 1, 'Fear': -1, 'Extreme Fear': -2, 'Extreme Greed': 2}\n",
    "\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Adjusting fng classification values\n",
    "# def adjust_df_for_ml(df:pd.DataFrame, map:dict) -> pd.DataFrame:\n",
    "#     df['fng_class_yest_adjusted'] = df['fng_class_yest'].map(map)\n",
    "#     df.drop(columns=['fng_class', 'fng_class_yest'], axis=1, inplace=True)\n",
    "#     return df\n",
    "\n",
    "# ml_data = adjust_df_for_ml(df=new_data, map=map)\n",
    "# ml_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to test 3 approaches:\n",
    "\n",
    "1. without any normalization or scalling\n",
    "2. applying a normalization\n",
    "3. applying scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting data for gridsearch\n",
    "train = new_data.iloc[0:2000]\n",
    "test = new_data.iloc[2001:2400]\n",
    "\n",
    "## prepare x_train for models\n",
    "X_train = train[predictors]\n",
    "X_train_norm = normalize(train[predictors])\n",
    "X_train_scaled = std_scaler.fit_transform(train[predictors])\n",
    "\n",
    "## prepate x_test for models\n",
    "X_test = test[predictors]\n",
    "X_test_norm = normalize(test[predictors])\n",
    "X_test_scaled = std_scaler.fit_transform(test[predictors])\n",
    "\n",
    "## Target\n",
    "y_train = train['target']\n",
    "y_test = test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Grid Search CV to find best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search_cv(model, params, X, y):\n",
    "    search_cv = GridSearchCV(model, params)\n",
    "    search_cv.fit(X, y)\n",
    "    return search_cv.best_score_, \\\n",
    "           search_cv.best_params_, \\\n",
    "           search_cv.best_estimator_\n",
    "\n",
    "def print_grid_evaluation_report(best_score: float, best_params: dict) -> None:\n",
    "    print(\"----Evaluation Report----\\n\")\n",
    "    print(f\"Best Score Achieved: {best_score}\\n\")\n",
    "    print(f\"Best Params Found: {best_params}\\n\")\n",
    "    print(\"---------------------------------\\n\")\n",
    "\n",
    "def predict(train: pd.DataFrame, test: pd.DataFrame, \n",
    "            predictors: List[str], model, mode:str= 'raw') -> pd.DataFrame:\n",
    "    y_train = train[\"target\"]\n",
    "    X_train = train[predictors]\n",
    "    X_test = test[predictors]\n",
    "    \n",
    "    if mode == 'norm':\n",
    "        X_train = normalize(X_train)\n",
    "        X_test = normalize(X_test)\n",
    "    elif mode == 'sca':\n",
    "        X_train = std_scaler.fit_transform(X_train)\n",
    "        X_test = std_scaler.fit_transform(X_test)\n",
    "    model.fit(X_train, y_train)\n",
    "    # preds = model.predict(X_test)\n",
    "    # Predict probabilities\n",
    "    probabilities = model.predict_proba(X_test)\n",
    "\n",
    "    # Extract probabilities for class 1\n",
    "    class_1_probs = probabilities[:, 1]  # Assuming class 1 is the second column\n",
    "\n",
    "    # Apply threshold\n",
    "    predictions = (class_1_probs >= 0.6).astype(int)\n",
    "    preds = pd.Series(predictions, index=test.index, name=\"predictions\")\n",
    "    return pd.concat([test[\"target\"], preds], axis=1)\n",
    "\n",
    "def backtest(data: pd.DataFrame, model, \n",
    "             predictors: List[str], start: int = 1095, \n",
    "             step: int = 150, mode:str = 'raw') -> pd.DataFrame:\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i + step)].copy()\n",
    "        predictions = predict(train, test, predictors, model, mode)\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "def evaluate_model(predictions: pd.DataFrame) -> Tuple[float, float]:\n",
    "    precision = precision_score(predictions[\"target\"], predictions[\"predictions\"])\n",
    "    accuracy = accuracy_score(predictions[\"target\"], predictions[\"predictions\"])\n",
    "    return precision, accuracy\n",
    "\n",
    "def plot_feature_importances(model, X:np.ndarray, y:np.ndarray, label:list) -> FeatureImportances:\n",
    "    fig_viz = FeatureImportances(model, labels=predictors)\n",
    "    fig_viz.fit(X, y)\n",
    "    return fig_viz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators':[100, 125, 150, 200],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [3, 5, 7, 9]\n",
    "}\n",
    "rf_model = RandomForestClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest (RF) raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_params, best_model_rf_cr = run_grid_search_cv(model=rf_model, params=rf_params, X=X_train, y=y_train)\n",
    "print_grid_evaluation_report(best_score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = backtest(new_data, best_model_rf_cr, predictors)\n",
    "\n",
    "# Avaliar o modelo\n",
    "precision, accuracy = evaluate_model(predictions)\n",
    "print(f\"Precisão do modelo: {precision:.2f}\")\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "feature_importance = plot_feature_importances(model=best_model_rf_cr, \n",
    "                                              X=X_train, \n",
    "                                              y=y_train,\n",
    "                                              label=predictors)\n",
    "plt.show(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest (RF) Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_params, best_model_rf_sca = run_grid_search_cv(model=rf_model, params=rf_params, X=X_train_scaled, y=y_train)\n",
    "print_grid_evaluation_report(best_score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = backtest(new_data, best_model_rf_sca, predictors, mode='sca')\n",
    "\n",
    "# Avaliar o modelo\n",
    "precision, accuracy = evaluate_model(predictions)\n",
    "print(f\"Precisão do modelo: {precision:.2f}\")\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "feature_importance = plot_feature_importances(model=best_model_rf_sca, \n",
    "                                              X=X_train_scaled, \n",
    "                                              y=y_train, \n",
    "                                              label = predictors)\n",
    "plt.show(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest (RF) Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_params, best_model_rf_norm = run_grid_search_cv(model=rf_model, params=rf_params, X=X_train_norm, y=y_train)\n",
    "print_grid_evaluation_report(best_score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = backtest(new_data, best_model_rf_norm, predictors, mode='norm')\n",
    "\n",
    "# Avaliar o modelo\n",
    "precision, accuracy = evaluate_model(predictions)\n",
    "print(f\"Precisão do modelo: {precision:.2f}\")\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "feature_importance = plot_feature_importances(model=best_model_rf_norm, \n",
    "                                              X=X_train_norm, \n",
    "                                              y=y_train, \n",
    "                                              label = predictors)\n",
    "plt.show(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':[5,7,9,10,11,12,13,14, 30, 60],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "knn_model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classifier Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_params, best_model_knn_cr = run_grid_search_cv(model=knn_model, params=knn_params, X=X_train, y=y_train)\n",
    "print_grid_evaluation_report(best_score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = backtest(new_data, best_model_knn_cr, predictors)\n",
    "\n",
    "# Avaliar o modelo\n",
    "precision, accuracy = evaluate_model(predictions)\n",
    "print(f\"Precisão do modelo: {precision:.2f}\")\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classifier Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_params, best_model_knn_sca = run_grid_search_cv(model=knn_model, params=knn_params, X=X_train_scaled, y=y_train)\n",
    "print_grid_evaluation_report(best_score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = backtest(new_data, best_model_knn_sca, predictors)\n",
    "\n",
    "# Avaliar o modelo\n",
    "precision, accuracy = evaluate_model(predictions)\n",
    "print(f\"Precisão do modelo: {precision:.2f}\")\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classifier Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_params, best_model_knn_norm = run_grid_search_cv(model=knn_model, params=knn_params, X=X_train_norm, y=y_train)\n",
    "print_grid_evaluation_report(best_score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = backtest(new_data, best_model_knn_norm, predictors, mode='norm')\n",
    "\n",
    "# Avaliar o modelo\n",
    "precision, accuracy = evaluate_model(predictions)\n",
    "print(f\"Precisão do modelo: {precision:.2f}\")\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'random_state' : [1], \n",
    "              'learning_rate' : [0.1, 0.2, 0.3, 0.4], \n",
    "              'n_estimators' : [100,150,200,250,300], \n",
    "              'colsample_bytree' : [0.25, 0.5, 0.75, 1], \n",
    "              'max_depth' : [3,5,6,7,8]\n",
    "}\n",
    "\n",
    "model_xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_params, best_model_xgb_cr = run_grid_search_cv(model=model_xgb, params=xgb_params, X=X_train, y=y_train)\n",
    "print_grid_evaluation_report(best_score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = backtest(new_data, best_model_xgb_cr, predictors)\n",
    "\n",
    "# Avaliar o modelo\n",
    "precision, accuracy = evaluate_model(predictions)\n",
    "print(f\"Precisão do modelo: {precision:.2f}\")\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "feature_importance = plot_feature_importances(model=best_model_xgb_cr, \n",
    "                                              X=X_train, \n",
    "                                              y=y_train,\n",
    "                                              label=predictors)\n",
    "plt.show(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_params, best_model_xgb_sca = run_grid_search_cv(model=model_xgb, params=xgb_params, X=X_train_scaled, y=y_train)\n",
    "print_grid_evaluation_report(best_score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = backtest(new_data, best_model_xgb_sca, predictors)\n",
    "\n",
    "# Avaliar o modelo\n",
    "precision, accuracy = evaluate_model(predictions)\n",
    "print(f\"Precisão do modelo: {precision:.2f}\")\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "feature_importance = plot_feature_importances(model=best_model_xgb_sca, \n",
    "                                              X=X_train_scaled, \n",
    "                                              y=y_train,\n",
    "                                              label=predictors)\n",
    "plt.show(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_params, best_model_xgb_norm = run_grid_search_cv(model=model_xgb, params=xgb_params, X=X_train_norm, y=y_train)\n",
    "print_grid_evaluation_report(best_score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = backtest(new_data, best_model_xgb_norm, predictors)\n",
    "\n",
    "# Avaliar o modelo\n",
    "precision, accuracy = evaluate_model(predictions)\n",
    "print(f\"Precisão do modelo: {precision:.2f}\")\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "feature_importance = plot_feature_importances(model=best_model_xgb_norm, \n",
    "                                              X=X_train_norm, \n",
    "                                              y=y_train,\n",
    "                                              label=predictors)\n",
    "plt.show(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Função de predição\n",
    "# def predict(train: pd.DataFrame, test: pd.DataFrame, predictors: List[str], model: XGBClassifier) -> pd.DataFrame:\n",
    "#     model.fit(train[predictors], train[\"target\"])\n",
    "#     preds = model.predict(test[predictors])\n",
    "#     preds = pd.Series(preds, index=test.index, name=\"predictions\")\n",
    "#     return pd.concat([test[\"target\"], preds], axis=1)\n",
    "\n",
    "# # Função de backtesting\n",
    "# # Usaremos uma abordagem de janela móvel para fazer o backtesting do modelo.\n",
    "# # Pegaremos uma janela de dados, usaremos para treinar o modelo e, em seguida, usaremos o modelo para prever a próxima janela de dados.\n",
    "# # Repetiremos esse processo até termos usado todos os dados.\n",
    "# # O parâmetro start significa 3 anos de dados para treinar o modelo.\n",
    "# # O parâmetro step significa 150 dias de dados para testar o modelo.\n",
    "# def backtest(data: pd.DataFrame, model: XGBClassifier, predictors: List[str], start: int = 1095, step: int = 150) -> pd.DataFrame:\n",
    "#     all_predictions = []\n",
    "\n",
    "#     for i in range(start, data.shape[0], step):\n",
    "#         train = data.iloc[0:i].copy()\n",
    "#         test = data.iloc[i:(i + step)].copy()\n",
    "#         predictions = predict(train, test, predictors, model)\n",
    "#         all_predictions.append(predictions)\n",
    "\n",
    "#     return pd.concat(all_predictions)\n",
    "\n",
    "# # Função de avaliação do modelo\n",
    "# def evaluate_model(predictions: pd.DataFrame) -> Tuple[float, float]:\n",
    "#     precision = precision_score(predictions[\"target\"], predictions[\"predictions\"])\n",
    "#     accuracy = accuracy_score(predictions[\"target\"], predictions[\"predictions\"])\n",
    "#     return precision, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Criar e treinar o modelo XGBoost\n",
    "# model = XGBClassifier(random_state=1, learning_rate=0.2, n_estimators=500, colsample_bytree = 1, max_depth = 8)\n",
    "# predictions = backtest(data, model, predictors)\n",
    "\n",
    "# # Avaliar o modelo\n",
    "# precision, accuracy = evaluate_model(predictions)\n",
    "# print(f\"Precisão do modelo: {precision:.2f}\")\n",
    "# print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialização do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o modelo treinado em um arquivo\n",
    "model_filename = \"btc_trend_prediction_model_best.joblib\"\n",
    "dump(best_model_rf_sca, model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
